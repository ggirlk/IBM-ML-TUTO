{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Build and test your first machine learning model using python and scikit-learn\n\nThere are many users of online trading platforms and these companies would like to run analytics on and predict churn based on user activity on the platform. Since competition is rife, keeping customers happy so they do not move their investments elsewhere is key to maintaining profitability.\n\nIn this notebook, we'll use scikit-learn to predict classes. scikit-learn provides implementations of many classification algorithms. In here, we have chosen the random forest classification algorithm to walk through all the different steps.\n\nTo help visualize what we are doing, we'll use 2D and 3D charts to show how the classes look (with 3 selected dimensions) with matplotlib and scikitplot python libraries.\n\n<a id=\"top\"></a>\n## Table of Contents\n\n1. [Load libraries](#load_libraries)\n2. [Data exploration](#explore_data)\n2. [Prepare data for building classification model](#prepare_data)\n3. [Split data into train and test sets](#split_data)\n4. [Helper methods for graph generation](#helper_methods)\n5. [Prepare Random Forest classification model](#prepare_model)\n6. [Train Random Forest classification model](#train_model)\n7. [Test Random Forest classification model](#test_model)\n8. [Evaluate Random Forest classification model](#evaluate_model)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Quick set of instructions to work through the notebook\n\nIf you are new to Notebooks, here's a quick overview of how to work in this environment.\n\n1. The notebook has 2 types of cells - markdown (text) such as this and code such as the one below. \n2. Each cell with code can be executed independently or together (see options under the Cell menu). When working in this notebook, we will be running one cell at a time because we need to make code changes to some of the cells.\n3. To run the cell, position cursor in the code cell and click the Run (arrow) icon. The cell is running when you see the * next to it. Some cells have printable output.\n4. Work through this notebook by reading the instructions and executing code cell by cell. Some cells will require modifications before you run them. "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id=\"load_libraries\"></a>\n## 1. Load libraries\n[Top](#top)\n\n\nInstall python modules\nNOTE! Some pip installs require a kernel restart.\nThe shell command pip install is used to install Python modules. Some installs require a kernel restart to complete. To avoid confusing errors, run the following cell once and then use the Kernel menu to restart the kernel before proceeding."
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Collecting pandas==0.24.2\n  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.1 MB 5.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pandas==0.24.2) (1.18.5)\nRequirement already satisfied: pytz>=2011k in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pandas==0.24.2) (2020.1)\nRequirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from python-dateutil>=2.5.0->pandas==0.24.2) (1.15.0)\n\u001b[31mERROR: ibm-watson-openscale 3.0.1 has requirement pandas>=0.25.3, but you'll have pandas 0.24.2 which is incompatible.\u001b[0m\nInstalling collected packages: pandas\n  Attempting uninstall: pandas\n    Found existing installation: pandas 1.0.5\n    Uninstalling pandas-1.0.5:\n      Successfully uninstalled pandas-1.0.5\nSuccessfully installed pandas-0.24.2\nCollecting pandas_ml==0.6.1\n  Downloading pandas_ml-0.6.1-py3-none-any.whl (100 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100 kB 6.7 MB/s ta 0:00:011\n\u001b[?25hCollecting enum34\n  Downloading enum34-1.1.10-py3-none-any.whl (11 kB)\nRequirement already satisfied: pandas>=0.19.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pandas_ml==0.6.1) (0.24.2)\nRequirement already satisfied: pytz>=2011k in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pandas>=0.19.0->pandas_ml==0.6.1) (2020.1)\nRequirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pandas>=0.19.0->pandas_ml==0.6.1) (2.8.1)\nRequirement already satisfied: numpy>=1.12.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pandas>=0.19.0->pandas_ml==0.6.1) (1.18.5)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from python-dateutil>=2.5.0->pandas>=0.19.0->pandas_ml==0.6.1) (1.15.0)\nInstalling collected packages: enum34, pandas-ml\nSuccessfully installed enum34-1.1.10 pandas-ml-0.6.1\nCollecting matplotlib==3.1.0\n  Downloading matplotlib-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (13.1 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13.1 MB 8.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy>=1.11 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from matplotlib==3.1.0) (1.18.5)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from matplotlib==3.1.0) (2.4.7)\nRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from matplotlib==3.1.0) (2.8.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from matplotlib==3.1.0) (0.10.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from matplotlib==3.1.0) (1.2.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib==3.1.0) (1.15.0)\nInstalling collected packages: matplotlib\n  Attempting uninstall: matplotlib\n    Found existing installation: matplotlib 3.2.2\n    Uninstalling matplotlib-3.2.2:\n      Successfully uninstalled matplotlib-3.2.2\nSuccessfully installed matplotlib-3.1.0\nCollecting scikit-learn==0.21.3\n  Downloading scikit_learn-0.21.3-cp37-cp37m-manylinux1_x86_64.whl (6.7 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.7 MB 6.2 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: joblib>=0.11 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from scikit-learn==0.21.3) (0.16.0)\nRequirement already satisfied: scipy>=0.17.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from scikit-learn==0.21.3) (1.5.0)\nRequirement already satisfied: numpy>=1.11.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from scikit-learn==0.21.3) (1.18.5)\nInstalling collected packages: scikit-learn\nSuccessfully installed scikit-learn-0.21.3\n"
                }
            ],
            "source": "!pip install pandas==0.24.2\n!pip install --user pandas_ml==0.6.1\n#downgrade matplotlib to bypass issue with confusion matrix being chopped out\n!pip install matplotlib==3.1.0\n!pip install --user scikit-learn==0.21.3\n!pip install -q scikit-plot"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ModuleNotFoundError",
                    "evalue": "No module named 'scikitplot'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-1-272d04c288f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolors\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmcolors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatches\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpatches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscikitplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mskplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
                        "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scikitplot'"
                    ]
                }
            ],
            "source": "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\n\nimport pandas as pd, numpy as np\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.colors as mcolors\nimport matplotlib.patches as mpatches\nimport scikitplot as skplt"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id=\"explore_data\"></a>\n## 2. Data exploration\n[Top](#top)\n\nData can be easily loaded within IBM Watson Studio. Instructions to load data within IBM Watson Studio can be found [here](https://ibmdev1.rtp.raleigh.ibm.com/tutorials/watson-studio-using-jupyter-notebook/). The data set can be located by its name and inserted into the notebook as a pandas DataFrame as shown below.\n\n![insert_spark_dataframe.png](https://raw.githubusercontent.com/IBM/icp4d-customer-churn-classifier/master/doc/source/images/insert_spark_dataframe.png)\n\nThe generated code comes up with a generic name and it is good practice to rename the dataframe to match the use case context.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>CHURNRISK</th>\n      <th>GENDER</th>\n      <th>STATUS</th>\n      <th>CHILDREN</th>\n      <th>ESTINCOME</th>\n      <th>HOMEOWNER</th>\n      <th>AGE</th>\n      <th>TOTALDOLLARVALUETRADED</th>\n      <th>TOTALUNITSTRADED</th>\n      <th>LARGESTSINGLETRANSACTION</th>\n      <th>SMALLESTSINGLETRANSACTION</th>\n      <th>PERCENTCHANGECALCULATION</th>\n      <th>DAYSSINCELASTLOGIN</th>\n      <th>DAYSSINCELASTTRADE</th>\n      <th>NETREALIZEDGAINS_YTD</th>\n      <th>NETREALIZEDLOSSES_YTD</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1703</td>\n      <td>Medium</td>\n      <td>NaN</td>\n      <td>M</td>\n      <td>2</td>\n      <td>28766.9</td>\n      <td>N</td>\n      <td>47</td>\n      <td>6110.61</td>\n      <td>58</td>\n      <td>1527.6525</td>\n      <td>152.76525</td>\n      <td>8.70</td>\n      <td>2</td>\n      <td>13</td>\n      <td>0.0000</td>\n      <td>152.76525</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1704</td>\n      <td>Low</td>\n      <td>NaN</td>\n      <td>M</td>\n      <td>2</td>\n      <td>91272.2</td>\n      <td>Y</td>\n      <td>25</td>\n      <td>26992.70</td>\n      <td>13</td>\n      <td>13496.3500</td>\n      <td>1349.63500</td>\n      <td>3.25</td>\n      <td>4</td>\n      <td>10</td>\n      <td>1349.6350</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1705</td>\n      <td>Low</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>0</td>\n      <td>73228.3</td>\n      <td>N</td>\n      <td>42</td>\n      <td>22472.25</td>\n      <td>28</td>\n      <td>11236.1250</td>\n      <td>1123.61250</td>\n      <td>7.00</td>\n      <td>4</td>\n      <td>5</td>\n      <td>1123.6125</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1706</td>\n      <td>High</td>\n      <td>M</td>\n      <td>M</td>\n      <td>1</td>\n      <td>64792.3</td>\n      <td>N</td>\n      <td>52</td>\n      <td>13051.31</td>\n      <td>36</td>\n      <td>6525.6550</td>\n      <td>652.56550</td>\n      <td>9.00</td>\n      <td>3</td>\n      <td>6</td>\n      <td>0.0000</td>\n      <td>652.56550</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1707</td>\n      <td>High</td>\n      <td>F</td>\n      <td>S</td>\n      <td>0</td>\n      <td>93322.1</td>\n      <td>Y</td>\n      <td>40</td>\n      <td>29922.99</td>\n      <td>8</td>\n      <td>14961.4950</td>\n      <td>1496.14950</td>\n      <td>2.00</td>\n      <td>4</td>\n      <td>9</td>\n      <td>0.0000</td>\n      <td>1496.14950</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "     ID CHURNRISK GENDER STATUS  CHILDREN  ESTINCOME HOMEOWNER  AGE  \\\n0  1703    Medium    NaN      M         2    28766.9         N   47   \n1  1704       Low    NaN      M         2    91272.2         Y   25   \n2  1705       Low    NaN      S         0    73228.3         N   42   \n3  1706      High      M      M         1    64792.3         N   52   \n4  1707      High      F      S         0    93322.1         Y   40   \n\n   TOTALDOLLARVALUETRADED  TOTALUNITSTRADED  LARGESTSINGLETRANSACTION  \\\n0                 6110.61                58                 1527.6525   \n1                26992.70                13                13496.3500   \n2                22472.25                28                11236.1250   \n3                13051.31                36                 6525.6550   \n4                29922.99                 8                14961.4950   \n\n   SMALLESTSINGLETRANSACTION  PERCENTCHANGECALCULATION  DAYSSINCELASTLOGIN  \\\n0                  152.76525                      8.70                   2   \n1                 1349.63500                      3.25                   4   \n2                 1123.61250                      7.00                   4   \n3                  652.56550                      9.00                   3   \n4                 1496.14950                      2.00                   4   \n\n   DAYSSINCELASTTRADE  NETREALIZEDGAINS_YTD  NETREALIZEDLOSSES_YTD  \n0                  13                0.0000              152.76525  \n1                  10             1349.6350                0.00000  \n2                   5             1123.6125                0.00000  \n3                   6                0.0000              652.56550  \n4                   9                0.0000             1496.14950  "
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "df_churn_pd = pd.read_csv(\n    \"https://raw.githubusercontent.com/IBM/ml-learning-path-assets/master/data/mergedcustomers_missing_values_GENDER.csv\")\ndf_churn_pd.head()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "\nprint(\"The dataset contains columns of the following data types : \\n\" +str(df_churn_pd.dtypes))\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Notice below that Gender has three missing values. This will be handled in one of the preprocessing steps that is to follow. "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "\nprint(\"The dataset contains following number of records for each of the columns : \\n\" +str(df_churn_pd.count()))\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "print( \"Each category within the churnrisk column has the following count : \")\nprint(df_churn_pd.groupby(['CHURNRISK']).size())\n#bar chart to show split of data\nindex = ['High','Medium','Low']\nchurn_plot = df_churn_pd['CHURNRISK'].value_counts(sort=True, ascending=False).plot(kind='bar',\n            figsize=(4,4),title=\"Total number for occurences of churn risk \" \n            + str(df_churn_pd['CHURNRISK'].count()), color=['#BB6B5A','#8CCB9B','#E5E88B'])\nchurn_plot.set_xlabel(\"Churn Risk\")\nchurn_plot.set_ylabel(\"Frequency\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id=\"prepare_data\"></a>\n## 3. Data preparation\n[Top](#top)\n\nData preparation is a very important step in machine learning model building. This is because the model can perform well only when the data it is trained on is good and well prepared. Hence, this step consumes the bulk of a data scientist's time spent building models.\n\nDuring this process, we identify categorical columns in the dataset. Categories need to be indexed, which means the string labels are converted to label indices. These label indices are encoded using One-hot encoding to a binary vector with at most a single value indicating the presence of a specific feature value from among the set of all feature values. This encoding allows algorithms which expect continuous features to use categorical features.\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "\n#remove columns that are not required\ndf_churn_pd = df_churn_pd.drop(['ID'], axis=1)\n\ndf_churn_pd.head()\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Defining the categorical columns \ncategoricalColumns = ['GENDER', 'STATUS', 'HOMEOWNER']\n\nprint(\"Categorical columns : \" )\nprint(categoricalColumns)\n\nimpute_categorical = SimpleImputer(strategy=\"most_frequent\")\n\nonehot_categorical =  OneHotEncoder(handle_unknown='ignore')\n\ncategorical_transformer = Pipeline(steps=[('impute',impute_categorical),('onehot',onehot_categorical)])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Defining the numerical columns \nnumericalColumns = df_churn_pd.select_dtypes(include=[np.float,np.int]).columns\n\nprint(\"Numerical columns : \" )\nprint(numericalColumns)\n\nscaler_numerical = StandardScaler()\n\nnumerical_transformer = Pipeline(steps=[('scale',scaler_numerical)])\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "preprocessorForCategoricalColumns = ColumnTransformer(transformers=[('cat', categorical_transformer, \n                                                                     categoricalColumns)],\n                                            remainder=\"passthrough\")\npreprocessorForAllColumns = ColumnTransformer(transformers=[('cat', categorical_transformer, categoricalColumns),\n                                                            ('num',numerical_transformer,numericalColumns)],\n                                            remainder=\"passthrough\")\n\n\n# The transformation happens in the pipeline. Temporarily done here to show what intermediate value looks like.\ndf_churn_pd_temp = preprocessorForCategoricalColumns.fit_transform(df_churn_pd)\nprint(\"Data after transforming :\")\nprint(df_churn_pd_temp)\n\ndf_churn_pd_temp_2 = preprocessorForAllColumns.fit_transform(df_churn_pd)\nprint(\"Data after transforming :\")\nprint(df_churn_pd_temp_2)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "# prepare data frame for splitting data into train and test datasets\n\nfeatures = []\nfeatures = df_churn_pd.drop(['CHURNRISK'], axis=1)\n\nlabel_churn = pd.DataFrame(df_churn_pd, columns = ['CHURNRISK']) \nlabel_encoder = LabelEncoder()\nlabel = df_churn_pd['CHURNRISK']\n\nlabel = label_encoder.fit_transform(label)\nprint(\"Encoded value of Churnrisk after applying label encoder : \" + str(label))\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "area = 75\nx = df_churn_pd['ESTINCOME']\ny = df_churn_pd['DAYSSINCELASTTRADE']\nz = df_churn_pd['TOTALDOLLARVALUETRADED']\n\npop_a = mpatches.Patch(color='#BB6B5A', label='High')\npop_b = mpatches.Patch(color='#E5E88B', label='Medium')\npop_c = mpatches.Patch(color='#8CCB9B', label='Low')\ndef colormap(risk_list):\n    cols=[]\n    for l in risk_list:\n        if l==0:\n            cols.append('#BB6B5A')\n        elif l==2:\n            cols.append('#E5E88B')\n        elif l==1:\n            cols.append('#8CCB9B')\n    return cols\n\nfig = plt.figure(figsize=(12,6))\nfig.suptitle('2D and 3D view of churnrisk data')\n\n# First subplot\nax = fig.add_subplot(1, 2,1)\n\nax.scatter(x, y, alpha=0.8, c=colormap(label), s= area)\nax.set_ylabel('DAYS SINCE LAST TRADE')\nax.set_xlabel('ESTIMATED INCOME')\n\nplt.legend(handles=[pop_a,pop_b,pop_c])\n\n# Second subplot\nax = fig.add_subplot(1,2,2, projection='3d')\n\nax.scatter(z, x, y, c=colormap(label), marker='o')\n\nax.set_xlabel('TOTAL DOLLAR VALUE TRADED')\nax.set_ylabel('ESTIMATED INCOME')\nax.set_zlabel('DAYS SINCE LAST TRADE')\n\nplt.legend(handles=[pop_a,pop_b,pop_c])\n\nplt.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n<a id=\"split_data\"></a>\n## 4. Split data into test and train\n[Top](#top)\n\nScikit-learn provides in built API to split the original dataset into train and test datasets. random_state is set to a number to be able to reproduce the same data split combination through multiple runs. "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "\nX_train, X_test, y_train, y_test = train_test_split(features,label , random_state=0)\nprint(\"Dimensions of datasets that will be used for training : Input features\"+str(X_train.shape)+ \n      \" Output label\" + str(y_train.shape))\nprint(\"Dimensions of datasets that will be used for testing : Input features\"+str(X_test.shape)+ \n      \" Output label\" + str(y_test.shape))\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id=\"helper_methods\"></a>\n## 5. Helper methods for graph generation\n[Top](#top)\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def colormap(risk_list):\n    cols=[]\n    for l in risk_list:\n        if l==0:\n            cols.append('#BB6B5A')\n        elif l==2:\n            cols.append('#E5E88B')\n        elif l==1:\n            cols.append('#8CCB9B')\n    return cols\n\ndef two_d_compare(y_test,y_pred,model_name):\n    #y_pred = label_encoder.fit_transform(y_pred)\n    #y_test = label_encoder.fit_transform(y_test)\n    area = (12 * np.random.rand(40))**2 \n    plt.subplots(ncols=2, figsize=(10,4))\n    plt.suptitle('Actual vs Predicted data : ' +model_name + '. Accuracy : %.2f' % accuracy_score(y_test, y_pred))\n\n    plt.subplot(121)\n    plt.scatter(X_test['ESTINCOME'], X_test['DAYSSINCELASTTRADE'], alpha=0.8, c=colormap(y_test))\n    plt.title('Actual')\n    plt.legend(handles=[pop_a,pop_b,pop_c])\n\n    plt.subplot(122)\n    plt.scatter(X_test['ESTINCOME'], X_test['DAYSSINCELASTTRADE'],alpha=0.8, c=colormap(y_pred))\n    plt.title('Predicted')\n    plt.legend(handles=[pop_a,pop_b,pop_c])\n\n    plt.show()\n    \nx = X_test['TOTALDOLLARVALUETRADED']\ny = X_test['ESTINCOME']\nz = X_test['DAYSSINCELASTTRADE']\n\npop_a = mpatches.Patch(color='#BB6B5A', label='High')\npop_b = mpatches.Patch(color='#E5E88B', label='Medium')\npop_c = mpatches.Patch(color='#8CCB9B', label='Low')\n\ndef three_d_compare(y_test,y_pred,model_name):\n    fig = plt.figure(figsize=(12,10))\n    fig.suptitle('Actual vs Predicted (3D) data : ' +model_name + '. Accuracy : %.2f' % accuracy_score(y_test, y_pred))\n    \n    ax = fig.add_subplot(121, projection='3d')\n    ax.scatter(x, y, z, c=colormap(y_test), marker='o')\n    ax.set_xlabel('TOTAL DOLLAR VALUE TRADED')\n    ax.set_ylabel('ESTIMATED INCOME')\n    ax.set_zlabel('DAYS SINCE LAST TRADE')\n    plt.legend(handles=[pop_a,pop_b,pop_c])\n    plt.title('Actual')\n\n    ax = fig.add_subplot(122, projection='3d')\n    ax.scatter(x, y, z, c=colormap(y_pred), marker='o')\n    ax.set_xlabel('TOTAL DOLLAR VALUE TRADED')\n    ax.set_ylabel('ESTIMATED INCOME')\n    ax.set_zlabel('DAYS SINCE LAST TRADE')\n    plt.legend(handles=[pop_a,pop_b,pop_c])\n    plt.title('Predicted')\n\n    plt.show()\n    \n\ndef model_metrics(y_test,y_pred):\n    print(\"Decoded values of Churnrisk after applying inverse of label encoder : \" + str(np.unique(y_pred)))\n\n    skplt.metrics.plot_confusion_matrix(y_test,y_pred,text_fontsize=\"small\",cmap='Greens',figsize=(6,4))\n    plt.show()\n    \n    print(\"The classification report for the model : \\n\\n\"+ classification_report(y_test, y_pred))\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id=\"prepare_model\"></a>\n## 6. Prepare Random Forest classification model\n[Top](#top)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We instantiate a decision-tree based classification algorithm, namely, RandomForestClassifier. Next we define a pipeline to chain together the various transformers and estimators defined during the data preparation step before. \nScikit-learn provides APIs that make it easier to combine multiple algorithms into a single pipeline.\n\nWe fit the pipeline to training data and apply the trained model to transform test data and generate churn risk class prediction."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from sklearn.ensemble import RandomForestClassifier\n\nmodel_name = \"Random Forest Classifier\"\n\nrandomForestClassifier = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "rfc_model = Pipeline(steps=[('preprocessorAll',preprocessorForAllColumns),('classifier', randomForestClassifier)])"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id=\"train_model\"></a>\n## 7. Train Random Forest classification model\n[Top](#top)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": false
            },
            "outputs": [],
            "source": "# Build models\n\nrfc_model.fit(X_train,y_train)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id=\"test_model\"></a>\n## 8. Test Random Forest classification model\n[Top](#top)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "\ny_pred_rfc = rfc_model.predict(X_test)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id=\"evaluate_model\"></a>\n## 9. Evaluate Random Forest classification model\n[Top](#top)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Model results\n\nIn a supervised classification problem such as churn risk classification, we have a true output and a model-generated predicted output for each data point. For this reason, the results for each data point can be assigned to one of four categories:\n\n1. True Positive (TP) - label is positive and prediction is also positive\n2. True Negative (TN) - label is negative and prediction is also negative\n3. False Positive (FP) - label is negative but prediction is positive\n4. False Negative (FN) - label is positive but prediction is negative\n\nThese four numbers are the building blocks for most classifier evaluation metrics. A fundamental point when considering classifier evaluation is that pure accuracy (i.e. was the prediction correct or incorrect) is not generally a good metric. The reason for this is because a dataset may be highly unbalanced. For example, if a model is designed to predict fraud from a dataset where 95% of the data points are not fraud and 5% of the data points are fraud, then a naive classifier that predicts not fraud, regardless of input, will be 95% accurate. For this reason, metrics like precision and recall are typically used because they take into account the type of error. In most applications there is some desired balance between precision and recall, which can be captured by combining the two into a single metric, called the F-measure.\n\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "two_d_compare(y_test,y_pred_rfc,model_name)\n\n# three_d_compare(y_test,y_pred_rfc,model_name)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Confusion matrix \n\nIn the graph below we have printed a confusion matrix and a self-explanotary classification report.\n\nThe confusion matrix shows that, 42 mediums were wrongly predicted as high, 2 mediums were wrongly predicted as low and 52 mediums were accurately predicted as mediums."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "y_test = label_encoder.inverse_transform(y_test)\ny_pred_rfc = label_encoder.inverse_transform(y_pred_rfc)\nmodel_metrics(y_test,y_pred_rfc)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Comparative study\nIn the bar chart below, we have compared the random forest classification algorithm output classes against the actual values. "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "uniqueValues, occurCount = np.unique(y_test, return_counts=True)\nfrequency_actual = (occurCount[0],occurCount[2],occurCount[1])\n\nuniqueValues, occurCount = np.unique(y_pred_rfc, return_counts=True)\nfrequency_predicted_rfc = (occurCount[0],occurCount[2],occurCount[1])\n\nn_groups = 3\nfig, ax = plt.subplots(figsize=(10,5))\nindex = np.arange(n_groups)\nbar_width = 0.1\nopacity = 0.8\n\nrects1 = plt.bar(index, frequency_actual, bar_width,\nalpha=opacity,\ncolor='g',\nlabel='Actual')\n\nrects6 = plt.bar(index + bar_width, frequency_predicted_rfc, bar_width,\nalpha=opacity,\ncolor='purple',\nlabel='Random Forest - Predicted')\n\nplt.xlabel('Churn Risk')\nplt.ylabel('Frequency')\nplt.title('Actual vs Predicted frequency.')\nplt.xticks(index + bar_width, ('High', 'Medium', 'Low'))\nplt.legend()\n\nplt.tight_layout()\nplt.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<p><font size=-1 color=gray>\n&copy; Copyright 2019 IBM Corp. All Rights Reserved.\n<p>\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file\nexcept in compliance with the License. You may obtain a copy of the License at\nhttps://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the\nLicense is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\nexpress or implied. See the License for the specific language governing permissions and\nlimitations under the License.\n</font></p>"
        }
    ],
    "metadata": {
        "celltoolbar": "Raw Cell Format",
        "kernelspec": {
            "display_name": "Python 3.7",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}